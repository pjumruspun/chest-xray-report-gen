{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2092lines [00:00, 174811.41lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Chexpert reward module...\n",
      "Using 1 GPUs!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_chest_xray_env import ChestXRayEnv\n",
    "from pytorch_dataset import ChestXRayDataset\n",
    "from pytorch_label import temperature_sampling\n",
    "from pytorch_tokenizer import create_tokenizer\n",
    "from pytorch_train import save_checkpoint, validate\n",
    "from utils import decode_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = 100\n",
    "temperature = 1.0\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "print_freq = 20\n",
    "accumulation_iter = 4\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(encoder, decoder, train_loader, tokenizer, envs, epoch, lr):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    reward_history = []\n",
    "    losses = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "\n",
    "    for i, (imgs, caps, _) in enumerate(tqdm(train_loader)):\n",
    "        # Actual batch size might not be equal to num_stances at the end of the dataset\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Reset the environment and set ground truth\n",
    "        for j in range(batch_size):\n",
    "            gt = caps[j].numpy()\n",
    "            envs[j].reset()\n",
    "            envs[j].set_ground_truth(gt)\n",
    "        \n",
    "        # Variables before running an opisode\n",
    "        dones = [False] * batch_size\n",
    "        res = [[] for _ in range(batch_size)]\n",
    "        ep_rewards = [[] for _ in range(batch_size)]\n",
    "        \n",
    "        seqs, _ = temperature_sampling(\n",
    "            encoder, decoder, tokenizer, images=imgs, \n",
    "            temperature=temperature, max_len=max_len, rl=True)\n",
    "\n",
    "        for j, (env, seq) in enumerate(zip(envs, seqs)):\n",
    "            for action in seq:\n",
    "                if not dones[j]:\n",
    "                    s, r, done, info = env.step(action)\n",
    "                    dones[j] = done\n",
    "                    last_nonzero_idx = np.max(s.nonzero())\n",
    "                    idx = int(s[last_nonzero_idx])\n",
    "                    res[j].append(tokenizer.itos[idx])\n",
    "                    ep_rewards[j].append(r)\n",
    "\n",
    "                if all(dones):\n",
    "                    break\n",
    "        \n",
    "        # Append results\n",
    "        final_rewards = [sum(ep_reward) for ep_reward in ep_rewards]\n",
    "        loss, policy_loss, value_loss = finish_episode(i, len(train_loader), decoder, ep_rewards, optimizer)\n",
    "        reward_history.extend(final_rewards)\n",
    "\n",
    "        if loss is not None:\n",
    "            losses.append(loss)\n",
    "            policy_losses.append(policy_loss)\n",
    "            value_losses.append(value_loss)\n",
    "            ith = i + len(train_loader) * epoch\n",
    "            tb.add_scalar(\"Episode_reward\", np.mean(final_rewards), ith)\n",
    "            tb.add_scalar(\"Loss\", loss, ith)\n",
    "            tb.add_scalar(\"Policy_loss\", policy_loss, ith)\n",
    "            tb.add_scalar(\"Value_loss\", value_loss, ith)\n",
    "            tb.add_text(\"Generated_sentence\", decode_sequences(tokenizer, seqs)[0], ith)\n",
    "\n",
    "\n",
    "def ragged_list_to_tensor(nested_ls, default_value):\n",
    "    # only works on 2D nested list\n",
    "    lengths = [len(e) for e in nested_ls]\n",
    "    max_list_len = max(lengths)\n",
    "    for ls in nested_ls:\n",
    "        to_fill = max_list_len - len(ls)\n",
    "        ls.extend([default_value for _ in range(to_fill)])\n",
    "\n",
    "    lengths = torch.tensor(lengths, device=device)\n",
    "    res = torch.tensor(nested_ls, device=device)\n",
    "\n",
    "    return res, lengths\n",
    "\n",
    "\n",
    "def finish_episode(i, loader_length, decoder, ep_rewards, optimizer):\n",
    "    gamma = 0.99\n",
    "    total_policy_losses = torch.tensor([0.0], device=device, requires_grad=True)\n",
    "    total_value_losses = torch.tensor([0.0], device=device, requires_grad=True)\n",
    "    # Truncate saved_actions to a proper length\n",
    "\n",
    "    # Convert to padded tensors\n",
    "    model_rewards, r_lengths = ragged_list_to_tensor(ep_rewards, default_value=float('nan'))\n",
    "    saved_actions, s_lengths = ragged_list_to_tensor(decoder.saved_actions, default_value=torch.tensor([float('nan'), float('nan')], device=device, requires_grad=True))\n",
    "\n",
    "    model_rewards.to(device)\n",
    "    saved_actions.to(device)\n",
    "\n",
    "    # Sort by size, descending\n",
    "    r_lengths, r_sort_ind = r_lengths.sort(dim=0, descending=True)\n",
    "    s_lengths, s_sort_ind = s_lengths.sort(dim=0, descending=True)\n",
    "    model_rewards = model_rewards[r_sort_ind] # (batch_size, max_seq_len)\n",
    "    saved_actions = saved_actions[s_sort_ind] # (batch_size, max_seq_len)\n",
    "\n",
    "    # Transpose so we can loop through each token\n",
    "    model_rewards = torch.t(model_rewards) # (max_seq_len, batch_size)\n",
    "    saved_actions = torch.transpose(saved_actions, 0, 1) # (max_seq_len, batch_size)\n",
    "\n",
    "    # Accumulated reward batched\n",
    "    accu_rewards = torch.zeros(len(ep_rewards), device=device)\n",
    "    total_batch_size = 0\n",
    "\n",
    "    for i, (log_prob_values, rewards)  in enumerate(zip(saved_actions[:-1], model_rewards[:-1])):\n",
    "\n",
    "        batch_size = sum([l > i+1 for l in r_lengths]).item() # i+1 because we use values_next\n",
    "        accu_rewards = accu_rewards + gamma * rewards\n",
    "\n",
    "        # Get rid of padded values\n",
    "        log_probs, values = torch.t(log_prob_values)[:,:batch_size]\n",
    "        _, values_next = torch.t(saved_actions[i+1])[:,:batch_size]\n",
    "        rewards = rewards[:batch_size]\n",
    "\n",
    "        # Calculate advantage\n",
    "        adv = rewards + gamma * values_next - values\n",
    "\n",
    "        # Calculate policy losses\n",
    "        policy_loss = -log_probs * adv\n",
    "\n",
    "        # Calculate value losses\n",
    "        # Huber loss, less fluctuative than squared loss\n",
    "        value_loss = F.smooth_l1_loss(values, accu_rewards[:batch_size])\n",
    "\n",
    "        # L = avg(log_prob * adv)\n",
    "        total_policy_losses = total_policy_losses + torch.sum(policy_loss)\n",
    "        total_value_losses = total_value_losses + torch.sum(value_loss)\n",
    "        \n",
    "        # Total batch size as divisor\n",
    "        total_batch_size += batch_size\n",
    "        \n",
    "    total_policy_losses = total_policy_losses\n",
    "    total_value_losses = total_value_losses\n",
    "    # print(total_batch_size)\n",
    "    loss = total_policy_losses + total_value_losses\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient accumulation\n",
    "    if (i + 1) % accumulation_iter == 0 or (i + 1) == loader_length:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # print(f\"policy_loss={[round(pl.item(), 2) for pl in total_policy_losses]}\\nvalue_loss={[round(vl.item(), 2) for vl in total_value_losses]}\\n{round(loss.item(), 2)=}\")\n",
    "    \n",
    "    return loss.item(), total_policy_losses.item(), total_value_losses.item()\n",
    "\n",
    "\n",
    "def get_expected_returns(rewards, gamma, normalize=True):\n",
    "    returns = []\n",
    "    g = 0 # Terminal step return = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        g = reward + gamma * g\n",
    "        returns.insert(0, g)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    if normalize and len(returns) > 1: # Prevent division by zero\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 15 19:04:04 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   43C    P8    13W / 210W |   3514MiB /  8192MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1064    C+G   ...\\app-1.0.9003\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      1240    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5200    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      5256    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      5276    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      6156    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      6460    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7680    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A      8000    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      8988    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      9364    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     10692    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12488    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     15272    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16808    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     17848    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     18004      C   ...vs\\research_rl\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     18136    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 12\n",
    "val_batch_size = 12\n",
    "epochs = 5\n",
    "tokenizer = create_tokenizer()\n",
    "sparse_reward = False\n",
    "num_instances = train_batch_size\n",
    "learning_rate = 1e-3\n",
    "metrics = 'f1'\n",
    "\n",
    "# Models\n",
    "checkpoint_path = 'weights\\pytorch_attention\\checkpoint_2022-02-07_20-50-35.370269.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "encoder = checkpoint['encoder']\n",
    "decoder = checkpoint['decoder']\n",
    "encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "epoch = checkpoint['epoch'] + 1\n",
    "print(f\"Using epoch {epoch} model...\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# DataLoader\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_loader = DataLoader(\n",
    "    ChestXRayDataset('train', transform=transforms.Compose([normalize])), \n",
    "    batch_size=train_batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ChestXRayDataset('val', transform=transforms.Compose([normalize])), \n",
    "    batch_size=val_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "# Create env (Parallel)\n",
    "envs = []\n",
    "for _ in range(num_instances):\n",
    "    env = ChestXRayEnv(tokenizer, max_len, sparse_reward=sparse_reward, metrics=metrics)\n",
    "    env.reset()\n",
    "    envs.append(env)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Finetuning epoch {epoch+1}\")\n",
    "    finetune(encoder, decoder, train_loader, tokenizer, envs, epoch, learning_rate)\n",
    "    # recent_bleu4 = validate(val_loader, encoder, decoder, loss_function, tokenizer)\n",
    "    save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, -1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5b2b1eb0b4180dccfbf3e31c6f887b3086317eead5a7f5406a6958b92b74dfc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('research_rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
