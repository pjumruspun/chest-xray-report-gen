{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2091lines [00:00, 190112.93lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caller: c:\\Users\\darkenstardragon\\Documents\\Work\\chest-xray-report-gen\\text_generation\\chexpert.py\n",
      "Creating Chexpert reward module...\n",
      "Using 1 GPUs!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from configs import configs\n",
    "from dataset import ChestXRayCaptionDataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import Chexnet\n",
    "from tqdm import tqdm\n",
    "from utils import train_transform, evaluate_transform, quantize_probs\n",
    "from tokenizer import create_tokenizer\n",
    "from test import evaluation_matrix\n",
    "from chexpert import chexpert\n",
    "import time\n",
    "import faiss\n",
    "from nlgeval import NLGEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_omit = [\n",
    "    'METEOR', \n",
    "    'SkipThoughtCS', \n",
    "    'EmbeddingAverageCosineSimilarity', \n",
    "    'VectorExtremaCosineSimilarity', \n",
    "    'GreedyMatchingScore', \n",
    "    'EmbeddingAverageCosineSimilairty',\n",
    "]\n",
    "\n",
    "nlgeval = NLGEval(metrics_to_omit=metrics_to_omit)  # loads the models\n",
    "\n",
    "LOAD_RANDOM_PROJECTION_DATA = False\n",
    "BUILD_CACHED_MAPS = False\n",
    "USE_CACHED_MAPS = True\n",
    "SAVE_PROJECTED = True\n",
    "time_file_path = 'results/inference_time.csv'\n",
    "nlg_file_path = 'results/results_nlg.csv'\n",
    "\n",
    "\n",
    "def filename(k, seed, project_dim):\n",
    "    d = {\n",
    "        'train_x': configs['mimic_dir'] + 'baseline_data/' +  f'train_image_embeddings_{project_dim}_{seed}.npy',\n",
    "        'train_y': configs['mimic_dir'] + 'baseline_data/' +  f'train_captions.npy',\n",
    "        'val_x': configs['mimic_dir'] +'baseline_data/' +  f'val_image_embeddings_{project_dim}_{seed}.npy',\n",
    "        'val_y': configs['mimic_dir'] +'baseline_data/' +  f'val_captions.npy',\n",
    "        'test_x': configs['mimic_dir'] +'baseline_data/' +  f'test_image_embeddings_{project_dim}_{seed}.npy',\n",
    "        'test_y': configs['mimic_dir'] +'baseline_data/' +  f'test_captions.npy',\n",
    "    }\n",
    "\n",
    "    return d[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2091lines [00:00, 174269.56lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded epoch 5 model, val_loss: 0.28202417492866516\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer()\n",
    "checkpoint = torch.load('weights/pretrained_encoder/pretrained_enc_epoch_5_2022-03-08_15-43-47.540586.pth.tar')\n",
    "print(f\"loaded epoch {checkpoint['epoch']+1} model, val_loss: {checkpoint['val_loss']}\")\n",
    "encoder = checkpoint['encoder'].cuda()\n",
    "\n",
    "train_probs_quantized = np.load(configs['mimic_dir'] + 'baseline_data/train_probs_quantized.npy')\n",
    "val_probs_quantized = np.load(configs['mimic_dir'] + 'baseline_data/val_probs_quantized.npy')\n",
    "test_probs_quantized = np.load(configs['mimic_dir'] + 'baseline_data/test_probs_quantized.npy')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ChestXRayCaptionDataset('train', transform=train_transform),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ChestXRayCaptionDataset('val', transform=evaluate_transform),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    ChestXRayCaptionDataset('test', transform=evaluate_transform),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "len_train_loader = 16740\n",
    "len_val_loader = 131\n",
    "len_test_loader = 229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_embeddings_random(encoder, data_loader, projection_matrix, project_every=2):\n",
    "    # With random projection\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "    captions = []\n",
    "    batch = []\n",
    "    with torch.no_grad():\n",
    "        for i, (img, caption, _) in enumerate(tqdm(data_loader)):\n",
    "            img = img.cuda()\n",
    "            encoded_img, _ = encoder(img)\n",
    "            batch.append(encoded_img.cpu())\n",
    "            captions.append(caption.cpu())\n",
    "            if ((i+1) % project_every) == 0 or (i+1) == len(data_loader):\n",
    "                batch = torch.cat(batch).reshape(-1, 1024*8*8).numpy()\n",
    "                batch = np.matmul(batch, projection_matrix)\n",
    "                image_embeddings.append(batch)\n",
    "                batch = []\n",
    "\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    captions = torch.cat(captions).numpy()\n",
    "    return image_embeddings, captions\n",
    "\n",
    "def generate_image_embeddings_save_every(encoder, data_split, data_loader, save_every=1024):\n",
    "    encoder.eval()\n",
    "    image_embeddings = []\n",
    "    captions = []\n",
    "    file_index = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, caption, _) in enumerate(tqdm(data_loader)):\n",
    "            img = img.cuda()\n",
    "            encoded_img, _ = encoder(img)\n",
    "            image_embeddings.append(encoded_img.cpu())\n",
    "            captions.append(caption.cpu())\n",
    "\n",
    "            if ((i+1) % save_every) == 0 or (i+1) == len(data_loader):\n",
    "                # stack\n",
    "                image_embeddings = torch.cat(image_embeddings).reshape(-1, 1024*8*8).numpy()\n",
    "                captions = torch.cat(captions).numpy()\n",
    "\n",
    "                # save\n",
    "                np.save(configs['mimic_dir'] + f'raw_embeddings/{data_split}/feature_maps_{file_index}.npy', image_embeddings)\n",
    "                np.save(configs['mimic_dir'] + f'raw_embeddings/{data_split}/captions_{file_index}.npy', captions)\n",
    "\n",
    "                # clear and update\n",
    "                image_embeddings = []\n",
    "                captions = []\n",
    "                file_index += 1\n",
    "\n",
    "def generate_probs_save_every(encoder, data_split, data_loader, save_every=1024):\n",
    "    encoder.eval()\n",
    "    probs = []\n",
    "    file_index = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, _, _) in enumerate(tqdm(data_loader)):\n",
    "            img = img.cuda()\n",
    "            _, prob = encoder(img)\n",
    "            probs.append(prob.cpu())\n",
    "\n",
    "            if ((i+1) % save_every) == 0 or (i+1) == len(data_loader):\n",
    "                # stack\n",
    "                probs = torch.cat(probs).numpy()\n",
    "\n",
    "                # save\n",
    "                np.save(configs['mimic_dir'] + f'raw_embeddings/{data_split}/probs_{file_index}.npy', probs)\n",
    "\n",
    "                # clear and update\n",
    "                probs = []\n",
    "                file_index += 1\n",
    "\n",
    "def random_project(len_data_loader, data_split, projection_matrix, save_every=1024, project_every=256):\n",
    "    n_split = len_data_loader // save_every + 1\n",
    "    print(f\"{n_split=}\")\n",
    "    projected_image_embeddings = []\n",
    "    captions = []\n",
    "    \n",
    "    for file_index in tqdm(range(n_split)):\n",
    "        feat_maps = np.load(configs['mimic_dir'] + f'raw_embeddings/{data_split}/feature_maps_{file_index}.npy')\n",
    "        caps = np.load(configs['mimic_dir'] + f'raw_embeddings/{data_split}/captions_{file_index}.npy')\n",
    "        captions.append(caps)\n",
    "\n",
    "        # project\n",
    "        feat_maps = np.array_split(feat_maps, project_every)\n",
    "        for batch in feat_maps:\n",
    "            proj = np.matmul(batch, projection_matrix)\n",
    "            projected_image_embeddings.append(proj)\n",
    "\n",
    "    projected_image_embeddings = np.vstack(projected_image_embeddings)\n",
    "    captions = np.vstack(captions)\n",
    "    \n",
    "    return projected_image_embeddings, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_probs_save_every(encoder, 'train', train_loader, save_every=1024)\n",
    "# generate_probs_save_every(encoder, 'val', val_loader, save_every=1024)\n",
    "# generate_probs_save_every(encoder, 'test', test_loader, save_every=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(SEED, RANDOM_PROJECT_DIM, load=False):\n",
    "    \"\"\"\n",
    "    End to end train, val, test projected vectors function\n",
    "    Input:\n",
    "        SEED (int): Seed of the random projection matrix\n",
    "        RANDOM_PROJECT_DIM: Dimension of the random projection matrix\n",
    "    Output:\n",
    "        train_x, train_y, val_x, val_y, test_x, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Load Cached Vectors\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        \n",
    "        file_suffix = f\"_{RANDOM_PROJECT_DIM}_{SEED}.npy\"\n",
    "        file_prefix = configs['mimic_dir'] + 'baseline_data/'\n",
    "        file_exists = os.path.exists(file_prefix + 'train_image_embeddings' + file_suffix)\n",
    "\n",
    "        if file_exists:\n",
    "            train_image_embeddings = np.load(file_prefix + 'train_image_embeddings' + file_suffix)\n",
    "            train_captions = np.load(file_prefix + 'train_captions.npy')\n",
    "            val_image_embeddings = np.load(file_prefix + 'val_image_embeddings' + file_suffix)\n",
    "            val_captions = np.load(file_prefix + 'val_captions.npy')\n",
    "            test_image_embeddings = np.load(file_prefix + 'test_image_embeddings' + file_suffix)\n",
    "            test_captions = np.load(file_prefix + 'test_captions.npy')\n",
    "            return train_image_embeddings, train_captions, val_image_embeddings, val_captions, test_image_embeddings, test_captions\n",
    "        else:\n",
    "            print(f\"Attempting to load file with seed={SEED} and project_dim={RANDOM_PROJECT_DIM} but doesn't exist\")\n",
    "    \n",
    "    print(f\"Random projecting with {SEED=}, {RANDOM_PROJECT_DIM=}\")\n",
    "    # Create a whole new projection\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    # Gaussian random projection\n",
    "    projection_matrix = rng.normal(0.0, 1/RANDOM_PROJECT_DIM, (65536, RANDOM_PROJECT_DIM))\n",
    "\n",
    "    \"\"\"\n",
    "    Train vectors\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Projecting train vectors...\")\n",
    "\n",
    "    if USE_CACHED_MAPS:\n",
    "        # New method: Predict first, cache them, then project\n",
    "        if BUILD_CACHED_MAPS:\n",
    "            generate_image_embeddings_save_every(encoder, 'train', train_loader, save_every=1024)\n",
    "\n",
    "        train_image_embeddings, train_captions = random_project(len_train_loader, 'train', projection_matrix, save_every=1024, project_every=64)\n",
    "        print(train_image_embeddings.shape)\n",
    "        print(train_captions.shape)\n",
    "        if SAVE_PROJECTED:\n",
    "            np.save(filename('train_x', seed=SEED, project_dim=RANDOM_PROJECT_DIM), train_image_embeddings)\n",
    "            np.save(filename('train_y', seed=SEED, project_dim=RANDOM_PROJECT_DIM), train_captions)\n",
    "    else:\n",
    "        # Old method: Project as we predict\n",
    "        if LOAD_RANDOM_PROJECTION_DATA:\n",
    "            # Use cached projection\n",
    "            # train_image_embeddings = np.load(filename['train_x'])\n",
    "            # train_captions = np.load(filename['train_y'])\n",
    "            print(train_image_embeddings.shape)\n",
    "            print(train_captions.shape)\n",
    "        else:\n",
    "            project_every = 256\n",
    "            train_image_embeddings, train_captions = generate_image_embeddings_random(encoder, train_loader, projection_matrix, project_every=project_every)\n",
    "            print(train_image_embeddings.shape)\n",
    "            print(train_captions.shape)\n",
    "            # np.save(filename['train_x'], train_image_embeddings)\n",
    "            # np.save(filename['train_y'], train_captions)\n",
    "    \n",
    "    \"\"\"\n",
    "    Val & Test vectors\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Projecting val & test vectors...\")\n",
    "\n",
    "    if USE_CACHED_MAPS:\n",
    "        # New method: Predict first, cache them, then project\n",
    "        if BUILD_CACHED_MAPS:\n",
    "            generate_image_embeddings_save_every(encoder, 'val', val_loader, save_every=1024)\n",
    "            generate_image_embeddings_save_every(encoder, 'test', test_loader, save_every=1024)\n",
    "\n",
    "        val_image_embeddings, val_captions = random_project(len_val_loader, 'val', projection_matrix, save_every=1024, project_every=64)\n",
    "        print(val_image_embeddings.shape)\n",
    "        print(val_captions.shape)\n",
    "        if SAVE_PROJECTED:\n",
    "            np.save(filename('val_x', seed=SEED, project_dim=RANDOM_PROJECT_DIM), val_image_embeddings)\n",
    "            np.save(filename('val_y', seed=SEED, project_dim=RANDOM_PROJECT_DIM), val_captions)\n",
    "        test_image_embeddings, test_captions = random_project(len_test_loader, 'test', projection_matrix, save_every=1024, project_every=64)\n",
    "        print(test_image_embeddings.shape)\n",
    "        print(test_captions.shape)\n",
    "        if SAVE_PROJECTED:\n",
    "            np.save(filename('test_x', seed=SEED, project_dim=RANDOM_PROJECT_DIM), test_image_embeddings)\n",
    "            np.save(filename('test_y', seed=SEED, project_dim=RANDOM_PROJECT_DIM), test_captions)\n",
    "\n",
    "    else:\n",
    "        if LOAD_RANDOM_PROJECTION_DATA:\n",
    "            # val_image_embeddings = np.load(filename['val_x'])\n",
    "            # val_captions = np.load(filename['val_y'])\n",
    "            # test_image_embeddings = np.load(filename['test_x'])\n",
    "            # test_captions = np.load(filename['test_y'])\n",
    "            print(val_image_embeddings.shape)\n",
    "            print(val_captions.shape)\n",
    "            print(test_image_embeddings.shape)\n",
    "            print(test_captions.shape)\n",
    "        else:\n",
    "            project_every = 256\n",
    "            val_image_embeddings, val_captions = generate_image_embeddings_random(encoder, val_loader, projection_matrix, project_every=project_every)\n",
    "            print(val_image_embeddings.shape)\n",
    "            print(val_captions.shape)\n",
    "            test_image_embeddings, test_captions = generate_image_embeddings_random(encoder, test_loader, projection_matrix, project_every=project_every)\n",
    "            print(test_image_embeddings.shape)\n",
    "            print(test_captions.shape)\n",
    "            # np.save(filename['val_x'], val_image_embeddings)\n",
    "            # np.save(filename['val_y'], val_captions)\n",
    "            # np.save(filename['test_x'], test_image_embeddings)\n",
    "            # np.save(filename['test_y'], test_captions)\n",
    "    \n",
    "    return train_image_embeddings, train_captions, val_image_embeddings, val_captions, test_image_embeddings, test_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilaritySearch:\n",
    "    def fit(self, xb, yb):\n",
    "        pass\n",
    "\n",
    "    def predict(self, xq):\n",
    "        pass\n",
    "\n",
    "class OneNearestNeighbor(SimilaritySearch):\n",
    "    def __init__(self):\n",
    "        self.yb = None\n",
    "        self.knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    def fit(self, xb, yb):\n",
    "        indices = [*range(xb.shape[0])]\n",
    "        self.knn.fit(xb.astype(np.float32), indices)\n",
    "        self.yb = yb.astype(np.float32)\n",
    "\n",
    "    def predict(self, xq):\n",
    "        dists, indices = self.knn.kneighbors(xq.astype(np.float32))\n",
    "        yq = np.array([self.yb[i] for i in indices])\n",
    "        yq = yq.reshape(xq.shape[0], self.yb.shape[1])\n",
    "        return yq\n",
    "\n",
    "class FaissFlatIndexL2CPU(SimilaritySearch):\n",
    "    def __init__(self):\n",
    "        self.yb = None\n",
    "        self.index = None\n",
    "\n",
    "    def fit(self, xb, yb):\n",
    "        dim = xb.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(xb.astype(np.float32))\n",
    "        self.yb = yb.astype(np.float32)\n",
    "\n",
    "    def predict(self, xq):\n",
    "        dists, indices = self.index.search(xq.astype(np.float32), 1)\n",
    "        yq = np.array([self.yb[i] for i in indices])\n",
    "        yq = yq.reshape(xq.shape[0], self.yb.shape[1])\n",
    "        return yq\n",
    "\n",
    "class FaissFlatIndexL2GPU(SimilaritySearch):\n",
    "    def __init__(self):\n",
    "        self.res = faiss.StandardGpuResources()\n",
    "        self.yb = None\n",
    "        self.gpu_index = None\n",
    "\n",
    "    def fit(self, xb, yb):\n",
    "        dim = xb.shape[1]\n",
    "        self.gpu_index = faiss.index_cpu_to_gpu(self.res, 0, faiss.IndexFlatL2(dim))\n",
    "        self.gpu_index.add(xb.astype(np.float32))\n",
    "        self.yb = yb.astype(np.float32)\n",
    "\n",
    "    def predict(self, xq):\n",
    "        dists, indices = self.gpu_index.search(xq.astype(np.float32), 1)\n",
    "        yq = np.array([self.yb[i] for i in indices])\n",
    "        yq = yq.reshape(xq.shape[0], self.yb.shape[1])\n",
    "        return yq\n",
    "\n",
    "class FaissHNSW32(SimilaritySearch):\n",
    "    def __init__(self):\n",
    "        self.yb = None\n",
    "        self.index = None\n",
    "\n",
    "    def fit(self, xb, yb):\n",
    "        dim = xb.shape[1]\n",
    "        self.index = faiss.IndexHNSWFlat(dim, 32)\n",
    "        self.index.add(xb.astype(np.float32))\n",
    "        self.yb = yb.astype(np.float32)\n",
    "\n",
    "    def predict(self, xq):\n",
    "        dists, indices = self.index.search(xq.astype(np.float32), 1)\n",
    "        yq = np.array([self.yb[i] for i in indices])\n",
    "        yq = yq.reshape(xq.shape[0], self.yb.shape[1])\n",
    "        return yq\n",
    "\n",
    "class FaissLSH32(SimilaritySearch):\n",
    "    def __init__(self):\n",
    "        self.yb = None\n",
    "        self.index = None\n",
    "\n",
    "    def fit(self, xb, yb):\n",
    "        dim = xb.shape[1]\n",
    "        self.index = faiss.IndexLSH(dim, 32)\n",
    "        self.index.add(xb.astype(np.float32))\n",
    "        self.yb = yb.astype(np.float32)\n",
    "\n",
    "    def predict(self, xq):\n",
    "        dists, indices = self.index.search(xq.astype(np.float32), 1)\n",
    "        yq = np.array([self.yb[i] for i in indices])\n",
    "        yq = yq.reshape(xq.shape[0], self.yb.shape[1])\n",
    "        return yq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilaritySearchCoarse2Fine(SimilaritySearch):\n",
    "    def assign_labels(self, train_labels):\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def assign_encoder(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "class OneNearestNeighborCoarse2Fine(SimilaritySearchCoarse2Fine):\n",
    "    def __init__(self):\n",
    "        # Dict label -> feature maps\n",
    "        self.map = defaultdict(lambda: [])\n",
    "        self.reports = defaultdict(lambda: [])\n",
    "        self.knns = dict()\n",
    "        self.assign_labels(train_probs_quantized)\n",
    "    \n",
    "    def fit(self, xb, yb):\n",
    "        binary_strings = [''.join(label.astype(str)) for label in self.train_labels]\n",
    "        # for label, feat_maps in tqdm(zip(binary_strings, xb), total=len(binary_strings)):\n",
    "        for i in range(len(binary_strings)):\n",
    "            label = binary_strings[i]\n",
    "            feat_maps = xb[i]\n",
    "            report = yb[i]\n",
    "            self.map[label].append(feat_maps)\n",
    "            self.reports[label].append(report)\n",
    "\n",
    "        for label, feat_maps_list in self.map.items():\n",
    "            indices = [*range(len(feat_maps_list))]\n",
    "            self.knns[label] = KNeighborsClassifier(n_neighbors=1)\n",
    "            self.knns[label].fit(np.array(feat_maps_list).astype(np.float32), indices)\n",
    "\n",
    "    def predict(self, xq, x_image):\n",
    "        _, probs = encoder(x_image)\n",
    "        labels = quantize_probs(probs.detach().cpu().numpy())\n",
    "        labels = [''.join(label.astype(str)) for label in labels]\n",
    "        results = []\n",
    "        no_label_count = 0\n",
    "        for label, feat_map in zip(labels, xq):\n",
    "            # Might not found exact label in training\n",
    "            # Might need to search in near edit distance\n",
    "            if label in self.knns.keys():\n",
    "                dists, index = self.knns[label].kneighbors(feat_map.astype(np.float32).reshape(1, -1))\n",
    "                yq = np.array(self.reports[label][index[0][0]]).reshape(-1)\n",
    "            else:\n",
    "                # Handle no label\n",
    "                similar_labels_list = self.get_similar_binaries(label)\n",
    "                similar_feature_maps = []\n",
    "                similar_reports = []\n",
    "                for sim_label in similar_labels_list:\n",
    "                    if sim_label in self.map.keys():\n",
    "                        # Need to make sure the sim label exists\n",
    "                        similar_feature_maps.extend(self.map[sim_label])\n",
    "                        similar_reports.extend(self.reports[sim_label])\n",
    "                \n",
    "                # Create temp knn\n",
    "                temp_knn = KNeighborsClassifier(n_neighbors=1)\n",
    "                indices = [*range(len(similar_feature_maps))]\n",
    "                temp_knn.fit(similar_feature_maps, indices)\n",
    "                dists, index = temp_knn.kneighbors(feat_map.astype(np.float32).reshape(1, -1))\n",
    "                yq = np.array(similar_reports[index[0][0]]).reshape(-1)\n",
    "                no_label_count += 1 # For monitoring purpose\n",
    "\n",
    "            results.append(yq)\n",
    "\n",
    "        # if no_label_count > 0:\n",
    "        #     print(f\"No label count: {no_label_count}\")\n",
    "        return np.array(results)\n",
    "\n",
    "    def get_similar_binaries(self, binary_str: str):\n",
    "        \"\"\"\n",
    "        Returns a list of similar binary string with edit distance = 1\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for i in range(len(binary_str)):\n",
    "            res.append(binary_str[:i] + str(int(not(bool(int(binary_str[i]))))) + binary_str[i+1:])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, embeddings, batch_size=64, image_loader=None):\n",
    "    \"\"\"\n",
    "    Predict captions given a similarity search model and image embeddings.\n",
    "\n",
    "    If the model is Coarse2Fine, an image_loader must be given for encoder\n",
    "    For predicting diseases in coarse searching step.\n",
    "\n",
    "    Parameters:\n",
    "        model (SimilaritySearch):                   Similarity search model to evaluate\n",
    "        embeddings (numpy.array):                   Image embeddings of size (sample_size, project_dim)\n",
    "        decode (bool):                              Whether to decode the report or not\n",
    "        batch_size (int):                           How many samples to do sim search per iteration\n",
    "        image_loader (torch.utils.data.DataLoader): DataLoader that can load images for encoder\n",
    "\n",
    "    Returns:\n",
    "        Predicted captions (numpy.array)\n",
    "        Total time to predict (float)\n",
    "    \"\"\"\n",
    "\n",
    "    captions = []\n",
    "    total_time = 0.0\n",
    "\n",
    "    # Check if model is Coarse2Fine\n",
    "    # image_loader must be given if so\n",
    "    assert isinstance(model, SimilaritySearchCoarse2Fine) == (image_loader != None), (\n",
    "        f\"isinstance={isinstance(model, SimilaritySearchCoarse2Fine)} but image_loader != None is {image_loader != None}\"\n",
    "    )\n",
    "\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine) and image_loader != None:\n",
    "        print(f\"forced batch size: {image_loader.batch_size}\")\n",
    "        data_loader = DataLoader(\n",
    "            embeddings,\n",
    "            batch_size=image_loader.batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        assert len(data_loader) == len(image_loader), (\n",
    "            f\"{len(data_loader)=}, {len(image_loader)=}\"\n",
    "        )\n",
    "\n",
    "        for j, (batch, (image, _, _)) in enumerate(tqdm(zip(data_loader, image_loader), total=len(data_loader))):\n",
    "            start = time.time()\n",
    "            yq = model.predict(batch.numpy(), image.cuda())\n",
    "            total_time += time.time() - start\n",
    "            captions.extend(yq)\n",
    "    else:\n",
    "        data_loader = DataLoader(\n",
    "            embeddings,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        for j, batch in enumerate(tqdm(data_loader)):\n",
    "            start = time.time()\n",
    "            yq = model.predict(batch.numpy())\n",
    "            total_time += time.time() - start\n",
    "            captions.extend(yq)\n",
    "\n",
    "    captions = np.array(captions).reshape(embeddings.shape[0], -1)\n",
    "\n",
    "    return captions, total_time\n",
    "\n",
    "def evaluate_clinical(true_captions, pred_captions, batch_size):\n",
    "    \"\"\"\n",
    "    Evaluate clinical accuracy of predicted reports vs ground truth\n",
    "    Using VisualCheXbert to compare between ground truth and predicted reports\n",
    "\n",
    "    Parameters:\n",
    "        true_captions (Iterable): Ground truth\n",
    "        pred_captions (Iterable): Predicted reports\n",
    "        batch_size (int): Size of the batch for VisualCheXbert to predict each iteration\n",
    "\n",
    "    Returns:\n",
    "        Evaluation matrix (Precision, recall, F1) of each disease and micro/macro avg (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "\n",
    "    true_df = []\n",
    "    pred_df = []\n",
    "    print(true_captions.shape[0], pred_captions.shape[0])\n",
    "    true_loader = DataLoader(\n",
    "        true_captions, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    pred_loader = DataLoader(\n",
    "        pred_captions, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    for t in tqdm(true_loader):\n",
    "        labels = chexpert(t, tokenizer)\n",
    "        true_df.append(labels)\n",
    "\n",
    "    for p in tqdm(pred_loader):\n",
    "        labels = chexpert(p, tokenizer)\n",
    "        pred_df.append(labels)\n",
    "    \n",
    "    true_df = pd.concat(true_df).reset_index(drop=True)\n",
    "    pred_df = pd.concat(pred_df).reset_index(drop=True)\n",
    "    return evaluation_matrix(true_df, pred_df)\n",
    "\n",
    "# def calculate_bleu_scores(true_captions, pred_captions):\n",
    "#     \"\"\"\n",
    "#     Calculates BLEU 1-4 scores based on NLTK functionality\n",
    "\n",
    "#     Parameters:\n",
    "#         true_captions: List of reference sentences\n",
    "#         pred_captions: List of generated sentences\n",
    "\n",
    "#     Returns:\n",
    "#         bleu_1, bleu_2, bleu_3, bleu_4: BLEU scores\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Put each sentence in references in a list\n",
    "#     # Because nltk accepts list of possible references for each sample\n",
    "#     true_captions = [[e.split()] for e in true_captions]\n",
    "#     pred_captions = [e.split() for e in pred_captions]\n",
    "\n",
    "#     bleu_1 = np.round(corpus_bleu(true_captions, pred_captions, weights=(1.0, 0., 0., 0.)), decimals=4)\n",
    "#     bleu_2 = np.round(corpus_bleu(true_captions, pred_captions, weights=(0.50, 0.50, 0., 0.)), decimals=4)\n",
    "#     bleu_3 = np.round(corpus_bleu(true_captions, pred_captions, weights=(0.33, 0.33, 0.33, 0.)), decimals=4)\n",
    "#     bleu_4 = np.round(corpus_bleu(true_captions, pred_captions, weights=(0.25, 0.25, 0.25, 0.25)), decimals=4)\n",
    "#     return bleu_1, bleu_2, bleu_3, bleu_4 \n",
    "\n",
    "def calculate_nlg_metrics(true_captions, pred_captions):\n",
    "    \"\"\"\n",
    "    Calculate BLEU 1-4, ROGUE_L, and CIDEr score using nlg-eval library\n",
    "    Parameters:\n",
    "        true_captions: List of reference sentences\n",
    "        pred_captions: List of generated sentences\n",
    "\n",
    "    Returns:\n",
    "        metrics_dict (dictionary): Dictionary containing all metrics above\n",
    "    \"\"\"\n",
    "\n",
    "    # nlg-eval requires user to do this to references\n",
    "    true_captions = [true_captions]\n",
    "    metrics_dict = nlgeval.compute_metrics(true_captions, pred_captions)\n",
    "    return metrics_dict\n",
    "\n",
    "def write_time_file(model, project_dim, seed, val_time, test_time):\n",
    "    with open(time_file_path, 'a') as f:\n",
    "        model_type = type(model).__name__\n",
    "        f.write(f\"{model_type},{pd.Timestamp.now()},{project_dim},{seed},{val_time},{test_time}\\n\")\n",
    "\n",
    "def write_nlg_file(model, project_dim, seed, split, metrics_dict):\n",
    "    with open(nlg_file_path, 'a') as f:\n",
    "        model_type = type(model).__name__\n",
    "        ls = [\n",
    "            model_type, pd.Timestamp.now(), project_dim, seed, split,\n",
    "            metrics_dict['Bleu_1'], metrics_dict['Bleu_2'], metrics_dict['Bleu_3'],\n",
    "            metrics_dict['Bleu_4'],metrics_dict['ROUGE_L'], metrics_dict['CIDEr'],\n",
    "        ]\n",
    "        # f.write((f\"{model_type},{pd.Timestamp.now()},{project_dim},{seed},{split},\"\n",
    "        # f\"{metrics_dict['Bleu_1']},{metrics_dict['Bleu_2']},{metrics_dict['Bleu_3']},\"\n",
    "        # f\"{metrics_dict['Bleu_4']},{metrics_dict['ROGUE_L']},{metrics_dict['CIDEr']}\\n\"\n",
    "        # ))\n",
    "        print(','.join([str(e) for e in ls]))\n",
    "        f.write(','.join([str(e) for e in ls]) + '\\n')\n",
    "\n",
    "def evaluate_all(model, val_image_embeddings, val_captions, test_image_embeddings, test_captions, seed, project_dim):\n",
    "    \"\"\"\n",
    "    Evaluate the model with val and test set. Will write results in directory results/.\n",
    "    Also will log time taken to predict with function write_time_file.\n",
    "\n",
    "    Metrics evaluated: \n",
    "        Clinical accuracy, \n",
    "        TODO: BLEU score\n",
    "\n",
    "    Parameters:\n",
    "        model (SimilaritySearch):           Similarity search model to evaluate\n",
    "        val_image_embeddings (numpy.array): Image embeddings of size (sample_size, project_dim)\n",
    "        val_captions (numpy.array):         Encoded captions of size (sample_size, max_caption_len)\n",
    "        test_image_embeddings (numpy.array): Image embeddings of size (sample_size, project_dim)\n",
    "        test_captions (numpy.array):        Encoded captions of size (sample_size, max_caption_len)\n",
    "        seed (int):                         (For logging purpose only) Seed used to random project\n",
    "        project_dim (int):                  Random project dimension\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        # Coarse2Fine models needs an image loader for the encoder to predict 14 diseases\n",
    "        predicted_reports, val_time = predict(model, val_image_embeddings, batch_size=1024, image_loader=val_loader)\n",
    "    else:\n",
    "        predicted_reports, val_time = predict(model, val_image_embeddings, batch_size=1024)\n",
    "    print(f\"Time taken to predict val: {val_time:.3f} seconds\")\n",
    "\n",
    "    # NLG Metrics\n",
    "    decoded_predicted_reports = tokenizer.decode(predicted_reports)\n",
    "    decoded_true_reports = tokenizer.decode(val_captions)\n",
    "    val_metrics_dict = calculate_nlg_metrics(decoded_true_reports, decoded_predicted_reports)\n",
    "    print(val_metrics_dict)\n",
    "    write_nlg_file(model, project_dim, seed, 'val', val_metrics_dict)\n",
    "\n",
    "    # Clinical Accuracy\n",
    "    # val_eval_matrix = evaluate_clinical(val_captions, predicted_reports, batch_size=8)\n",
    "\n",
    "    # Save Results to csv\n",
    "    # val_eval_matrix.to_csv(\n",
    "    #     f'results/{type(model).__name__}_val_results_{project_dim}_{seed}.csv', index=False\n",
    "    # )\n",
    "    # print(val_eval_matrix)\n",
    "\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        # Coarse2Fine models needs an image loader for the encoder to predict 14 diseases\n",
    "        predicted_reports, test_time = predict(model, test_image_embeddings, batch_size=1024, image_loader=test_loader)\n",
    "    else:\n",
    "        predicted_reports, test_time = predict(model, test_image_embeddings, batch_size=1024)\n",
    "    print(f\"Time taken to predict test: {test_time:.3f} seconds\")\n",
    "\n",
    "    # NLG Metrics\n",
    "    decoded_predicted_reports = tokenizer.decode(predicted_reports)\n",
    "    decoded_true_reports = tokenizer.decode(test_captions)\n",
    "    test_metrics_dict = calculate_nlg_metrics(decoded_true_reports, decoded_predicted_reports)\n",
    "    print(test_metrics_dict)\n",
    "    write_nlg_file(model, project_dim, seed, 'test', test_metrics_dict)\n",
    "\n",
    "    # Clinical Accuracy\n",
    "    # test_eval_matrix = evaluate_clinical(test_captions, predicted_reports, batch_size=8)\n",
    "\n",
    "    # Save Results to csv\n",
    "    # test_eval_matrix.to_csv(\n",
    "    #     f'results/{type(model).__name__}_test_results_{project_dim}_{seed}.csv', index=False\n",
    "    # )\n",
    "    # print(test_eval_matrix)\n",
    "\n",
    "    # Save time taken to csv\n",
    "    # write_time_file(model, project_dim, seed, val_time, test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_benchmark(model_class, seed, project_dim, load=False):\n",
    "    \"\"\"\n",
    "    Perform vector acquisition, 1-NN, and evaluate on val and test set, and save results into files\n",
    "    \"\"\"\n",
    "\n",
    "    # Get vector\n",
    "    train_image_embeddings, train_captions, val_image_embeddings, val_captions, test_image_embeddings, test_captions = get_vectors(seed, project_dim, load=load)\n",
    "\n",
    "    # Model selection\n",
    "    model = model_class()\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        model.assign_encoder(encoder)\n",
    "\n",
    "    # Train\n",
    "    model.fit(train_image_embeddings, train_captions)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_all(model, val_image_embeddings, val_captions, test_image_embeddings, test_captions, seed, project_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [OneNearestNeighbor, OneNearestNeighborCoarse2Fine]\n",
    "# seeds = [2000, 3000, 4000]\n",
    "# dims = [128, 256, 512, 1024, 2048,]\n",
    "\n",
    "# for model_class in models:\n",
    "#     for dim in dims:\n",
    "#         for seed in seeds:\n",
    "#             # path_to_check = f'results/{model_class.__name__}_test_results_{dim}_{seed}.csv'\n",
    "#             # exist = os.path.exists(path_to_check)\n",
    "#             # if not exist:\n",
    "#             print(model_class.__name__, dim, seed)\n",
    "#             e2e_benchmark(model_class, seed=seed, project_dim=dim, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneNearestNeighbor 4096 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:28<00:00, 29.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 86.169 seconds\n",
      "{'Bleu_1': 0.3831014493200462, 'Bleu_2': 0.2261327925554635, 'Bleu_3': 0.14586729140405622, 'Bleu_4': 0.09953168578521623, 'ROUGE_L': 0.2762005955467192, 'CIDEr': 0.15458831320661218}\n",
      "OneNearestNeighbor,2022-05-03 15:19:21.624131,4096,0,val,0.3831014493200462,0.2261327925554635,0.14586729140405622,0.09953168578521623,0.2762005955467192,0.15458831320661218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:15<00:00, 33.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 135.228 seconds\n",
      "{'Bleu_1': 0.3447961682021063, 'Bleu_2': 0.18715684503239993, 'Bleu_3': 0.10987197669793941, 'Bleu_4': 0.06877964076958654, 'ROUGE_L': 0.23495707900266835, 'CIDEr': 0.08226999209838405}\n",
      "OneNearestNeighbor,2022-05-03 15:21:55.772275,4096,0,test,0.3447961682021063,0.18715684503239993,0.10987197669793941,0.06877964076958654,0.23495707900266835,0.08226999209838405\n",
      "OneNearestNeighbor 8192 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:53<00:00, 57.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 169.451 seconds\n",
      "{'Bleu_1': 0.3871605490898203, 'Bleu_2': 0.22728770372520876, 'Bleu_3': 0.1460331847338342, 'Bleu_4': 0.09971030849304723, 'ROUGE_L': 0.2759508156633303, 'CIDEr': 0.15275302515361233}\n",
      "OneNearestNeighbor,2022-05-03 15:29:49.586626,8192,0,val,0.3871605490898203,0.22728770372520876,0.1460331847338342,0.09971030849304723,0.2759508156633303,0.15275302515361233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [04:19<00:00, 64.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 258.158 seconds\n",
      "{'Bleu_1': 0.3454086447677871, 'Bleu_2': 0.1883578218631788, 'Bleu_3': 0.11115490481256213, 'Bleu_4': 0.07010046875111835, 'ROUGE_L': 0.23542945147266883, 'CIDEr': 0.07968202935293578}\n",
      "OneNearestNeighbor,2022-05-03 15:34:26.001236,8192,0,test,0.3454086447677871,0.1883578218631788,0.11115490481256213,0.07010046875111835,0.23542945147266883,0.07968202935293578\n",
      "OneNearestNeighborCoarse2Fine 4096 0\n",
      "forced batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [22:09<00:00, 10.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 1319.073 seconds\n",
      "{'Bleu_1': 0.3867615369373467, 'Bleu_2': 0.22814281896641755, 'Bleu_3': 0.14698385870330452, 'Bleu_4': 0.10055462105665386, 'ROUGE_L': 0.27727018051677726, 'CIDEr': 0.1585761137936198}\n",
      "OneNearestNeighborCoarse2Fine,2022-05-03 15:58:01.377536,4096,0,val,0.3867615369373467,0.22814281896641755,0.14698385870330452,0.10055462105665386,0.27727018051677726,0.1585761137936198\n",
      "forced batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [13:43<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 812.476 seconds\n",
      "{'Bleu_1': 0.34280064358632384, 'Bleu_2': 0.18643096850531704, 'Bleu_3': 0.10990971009637332, 'Bleu_4': 0.0690075559338383, 'ROUGE_L': 0.2341157421352432, 'CIDEr': 0.07322299716053964}\n",
      "OneNearestNeighborCoarse2Fine,2022-05-03 16:11:59.562597,4096,0,test,0.34280064358632384,0.18643096850531704,0.10990971009637332,0.0690075559338383,0.2341157421352432,0.07322299716053964\n",
      "OneNearestNeighborCoarse2Fine 8192 0\n",
      "forced batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [44:16<00:00, 20.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 2649.305 seconds\n",
      "{'Bleu_1': 0.38634549491303666, 'Bleu_2': 0.2280552271136684, 'Bleu_3': 0.1470700774127665, 'Bleu_4': 0.10064868763305572, 'ROUGE_L': 0.27723266044993267, 'CIDEr': 0.15873037363879317}\n",
      "OneNearestNeighborCoarse2Fine,2022-05-03 16:59:43.407265,8192,0,val,0.38634549491303666,0.2280552271136684,0.1470700774127665,0.10064868763305572,0.27723266044993267,0.15873037363879317\n",
      "forced batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [26:55<00:00,  7.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 1602.994 seconds\n",
      "{'Bleu_1': 0.3434323042117288, 'Bleu_2': 0.18668214252042056, 'Bleu_3': 0.11002225961032984, 'Bleu_4': 0.0695910268170324, 'ROUGE_L': 0.23515237991127258, 'CIDEr': 0.07663434906770622}\n",
      "OneNearestNeighborCoarse2Fine,2022-05-03 17:26:54.667768,8192,0,test,0.3434323042117288,0.18668214252042056,0.11002225961032984,0.0695910268170324,0.23515237991127258,0.07663434906770622\n"
     ]
    }
   ],
   "source": [
    "models = [OneNearestNeighbor, OneNearestNeighborCoarse2Fine]\n",
    "seeds = [0]\n",
    "dims = [4096, 8192]\n",
    "\n",
    "for model_class in models:\n",
    "    for dim in dims:\n",
    "        for seed in seeds:\n",
    "            # path_to_check = f'results/{model_class.__name__}_test_results_{dim}_{seed}.csv'\n",
    "            # exist = os.path.exists(path_to_check)\n",
    "            # if not exist:\n",
    "            print(model_class.__name__, dim, seed)\n",
    "            e2e_benchmark(model_class, seed=seed, project_dim=dim, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chexpertify(captions, batch_size):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with labels and captions alongside\n",
    "    Example usage:\n",
    "\n",
    "        true_df = chexpertify(test_captions, batch_size=12)\n",
    "        pred_df = chexpertify(test_predicted_reports, batch_size=12)\n",
    "\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    data_loader = DataLoader(\n",
    "        captions, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        labels = chexpert(batch, tokenizer)\n",
    "        df.append(labels)\n",
    "\n",
    "\n",
    "    df = pd.concat(df).reset_index(drop=True)\n",
    "    df['captions'] = tokenizer.decode(captions)\n",
    "    return df\n",
    "\n",
    "def e2e_chexpertify(model_class, seed, project_dim, load=False):\n",
    "    \"\"\"\n",
    "    Perform vector acquisition, 1-NN, and evaluate on val and test set, and save results into files\n",
    "    \"\"\"\n",
    "\n",
    "    # Get vector\n",
    "    train_image_embeddings, train_captions, val_image_embeddings, val_captions, test_image_embeddings, test_captions = get_vectors(seed, project_dim, load=load)\n",
    "\n",
    "    # Model selection\n",
    "    model = model_class()\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        model.assign_encoder(encoder)\n",
    "\n",
    "    # Train\n",
    "    model.fit(train_image_embeddings, train_captions)\n",
    "\n",
    "    # Get val captions\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        # Coarse2Fine models needs an image loader for the encoder to predict 14 diseases\n",
    "        predicted_reports, val_time = predict(model, val_image_embeddings, batch_size=1024, image_loader=val_loader)\n",
    "    else:\n",
    "        predicted_reports, val_time = predict(model, val_image_embeddings, batch_size=1024)\n",
    "    print(f\"Time taken to predict val: {val_time:.3f} seconds\")\n",
    "\n",
    "    val_pred_df = chexpertify(predicted_reports, batch_size=12)\n",
    "    val_true_df = chexpertify(val_captions, batch_size=12)\n",
    "\n",
    "    # Save Results to csv\n",
    "    val_pred_df.to_csv(\n",
    "        f'chexpertify/{type(model).__name__}_val_pred_df_{project_dim}_{seed}.csv', index=False\n",
    "    )\n",
    "    val_true_df.to_csv(\n",
    "        f'chexpertify/{type(model).__name__}_val_true_df_{project_dim}_{seed}.csv', index=False\n",
    "    )\n",
    "    print(val_pred_df.head())\n",
    "    print(val_true_df.head())\n",
    "\n",
    "    # Get test captions\n",
    "    if isinstance(model, SimilaritySearchCoarse2Fine):\n",
    "        # Coarse2Fine models needs an image loader for the encoder to predict 14 diseases\n",
    "        predicted_reports, test_time = predict(model, test_image_embeddings, batch_size=1024, image_loader=test_loader)\n",
    "    else:\n",
    "        predicted_reports, test_time = predict(model, test_image_embeddings, batch_size=1024)\n",
    "    print(f\"Time taken to predict test: {test_time:.3f} seconds\")\n",
    "\n",
    "    test_pred_df = chexpertify(predicted_reports, batch_size=12)\n",
    "    test_true_df = chexpertify(test_captions, batch_size=12)\n",
    "\n",
    "    # Save Results to csv\n",
    "    test_pred_df.to_csv(\n",
    "        f'chexpertify/{type(model).__name__}_test_pred_df_{project_dim}_{seed}.csv', index=False\n",
    "    )\n",
    "    test_true_df.to_csv(\n",
    "        f'chexpertify/{type(model).__name__}_test_true_df_{project_dim}_{seed}.csv', index=False\n",
    "    )\n",
    "    print(test_pred_df.head())\n",
    "    print(test_true_df.head())\n",
    "\n",
    "def load_chexpertify_results(model_class, seed, project_dim):\n",
    "    val_pred_df = pd.read_csv(f'chexpertify/{type(model_class()).__name__}_val_pred_df_{project_dim}_{seed}.csv')\n",
    "    val_true_df = pd.read_csv(f'chexpertify/{type(model_class()).__name__}_val_true_df_{project_dim}_{seed}.csv')\n",
    "    test_pred_df = pd.read_csv(f'chexpertify/{type(model_class()).__name__}_test_pred_df_{project_dim}_{seed}.csv')\n",
    "    test_true_df = pd.read_csv(f'chexpertify/{type(model_class()).__name__}_test_true_df_{project_dim}_{seed}.csv')\n",
    "    return val_pred_df, val_true_df, test_pred_df, test_true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 9.380 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:28<00:00,  6.01it/s]\n",
      "100%|██████████| 174/174 [00:25<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         1.0           1.0           0.0          0.0    0.0   \n",
      "1                         0.0           0.0           0.0          0.0    0.0   \n",
      "2                         1.0           1.0           1.0          0.0    0.0   \n",
      "3                         1.0           1.0           1.0          0.0    1.0   \n",
      "4                         1.0           1.0           1.0          0.0    1.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          0.0           0.0               0.0   \n",
      "1            0.0        0.0          0.0           0.0               0.0   \n",
      "2            1.0        1.0          0.0           0.0               1.0   \n",
      "3            1.0        1.0          1.0           0.0               1.0   \n",
      "4            1.0        1.0          1.0           0.0               1.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            1.0       1.0              0.0         0.0   \n",
      "1            0.0       1.0              0.0         0.0   \n",
      "2            1.0       1.0              0.0         0.0   \n",
      "3            1.0       1.0              0.0         0.0   \n",
      "4            0.0       1.0              0.0         0.0   \n",
      "\n",
      "                                            captions  \n",
      "0  lungs remain hyperinflated . there is a large ...  \n",
      "1  there has been interval removal of a rightside...  \n",
      "2  compared with prior there has been no signific...  \n",
      "3  right moderate pleural effusion is worsened fr...  \n",
      "4  the monitoring and support have been removed ....  \n",
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         1.0           1.0           1.0          0.0    0.0   \n",
      "1                         1.0           1.0           1.0          0.0    0.0   \n",
      "2                         1.0           1.0           1.0          0.0    1.0   \n",
      "3                         1.0           1.0           1.0          0.0    1.0   \n",
      "4                         1.0           1.0           1.0          0.0    1.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          1.0           0.0               1.0   \n",
      "1            0.0        0.0          1.0           0.0               1.0   \n",
      "2            1.0        1.0          1.0           0.0               1.0   \n",
      "3            1.0        1.0          1.0           0.0               1.0   \n",
      "4            1.0        1.0          1.0           0.0               1.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            1.0       1.0              0.0         0.0   \n",
      "1            1.0       1.0              0.0         0.0   \n",
      "2            1.0       1.0              0.0         0.0   \n",
      "3            1.0       1.0              0.0         0.0   \n",
      "4            0.0       1.0              0.0         0.0   \n",
      "\n",
      "                                            captions  \n",
      "0  no evidence of consolidation to suggest pneumo...  \n",
      "1  no evidence of consolidation to suggest pneumo...  \n",
      "2  there are moderate bilateral pleural effusions...  \n",
      "3  there are moderate bilateral pleural effusions...  \n",
      "4  moderate to large bilateral pleural effusions ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 16.728 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:56<00:00,  5.40it/s]\n",
      "100%|██████████| 305/305 [00:50<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         0.0           0.0           1.0          0.0    0.0   \n",
      "1                         1.0           1.0           1.0          0.0    0.0   \n",
      "2                         0.0           0.0           0.0          0.0    0.0   \n",
      "3                         0.0           0.0           0.0          0.0    0.0   \n",
      "4                         0.0           0.0           0.0          0.0    0.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          1.0           0.0               0.0   \n",
      "1            0.0        0.0          1.0           0.0               0.0   \n",
      "2            0.0        0.0          0.0           0.0               0.0   \n",
      "3            0.0        0.0          0.0           0.0               0.0   \n",
      "4            0.0        0.0          0.0           0.0               0.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            0.0       0.0              0.0         0.0   \n",
      "1            0.0       1.0              0.0         0.0   \n",
      "2            0.0       0.0              0.0         1.0   \n",
      "3            0.0       0.0              0.0         1.0   \n",
      "4            0.0       0.0              0.0         1.0   \n",
      "\n",
      "                                            captions  \n",
      "0  there are low lung volumes and bibasilar opaci...  \n",
      "1  pa and lateral views of the chest . midline st...  \n",
      "2  pa and lateral chest radiographs dated no sign...  \n",
      "3  no focal opacity pulmonary edema or pneumothor...  \n",
      "4  there is no change . normal lung volumes . nor...  \n",
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         0.0           0.0           0.0          0.0    0.0   \n",
      "1                         0.0           0.0           0.0          0.0    0.0   \n",
      "2                         0.0           0.0           0.0          0.0    0.0   \n",
      "3                         0.0           0.0           0.0          0.0    0.0   \n",
      "4                         0.0           0.0           0.0          0.0    0.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          0.0           0.0               0.0   \n",
      "1            0.0        0.0          0.0           0.0               0.0   \n",
      "2            0.0        0.0          0.0           0.0               0.0   \n",
      "3            0.0        0.0          0.0           0.0               0.0   \n",
      "4            0.0        0.0          0.0           0.0               0.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            0.0       0.0              0.0         0.0   \n",
      "1            0.0       0.0              0.0         0.0   \n",
      "2            0.0       0.0              0.0         1.0   \n",
      "3            0.0       0.0              0.0         1.0   \n",
      "4            0.0       0.0              0.0         1.0   \n",
      "\n",
      "                                            captions  \n",
      "0  lateral view somewhat limited due to overlying...  \n",
      "1  lateral view somewhat limited due to overlying...  \n",
      "2  frontal and lateral radiographs of the chest a...  \n",
      "3  frontal and lateral radiographs of the chest a...  \n",
      "4  frontal and lateral radiographs of the chest a...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict val: 11.335 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:29<00:00,  5.80it/s]\n",
      "100%|██████████| 174/174 [00:25<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         1.0           1.0           1.0          0.0    1.0   \n",
      "1                         1.0           1.0           1.0          0.0    0.0   \n",
      "2                         1.0           1.0           1.0          0.0    1.0   \n",
      "3                         1.0           1.0           1.0          0.0    0.0   \n",
      "4                         1.0           1.0           1.0          0.0    1.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          0.0           0.0               0.0   \n",
      "1            1.0        1.0          1.0           0.0               1.0   \n",
      "2            1.0        1.0          1.0           0.0               1.0   \n",
      "3            0.0        0.0          1.0           0.0               1.0   \n",
      "4            1.0        1.0          1.0           0.0               1.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            0.0       0.0              0.0         0.0   \n",
      "1            1.0       1.0              0.0         0.0   \n",
      "2            0.0       1.0              0.0         0.0   \n",
      "3            0.0       1.0              0.0         0.0   \n",
      "4            1.0       1.0              0.0         0.0   \n",
      "\n",
      "                                            captions  \n",
      "0  there is moderate cardiomegaly and moderate va...  \n",
      "1  the right lung is well expanded there is impro...  \n",
      "2  there is a moderate pleural effusion on the ri...  \n",
      "3  blunting at the left costophrenic may represen...  \n",
      "4  the left pleural effusion is unchanged . on th...  \n",
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         1.0           1.0           1.0          0.0    0.0   \n",
      "1                         1.0           1.0           1.0          0.0    0.0   \n",
      "2                         1.0           1.0           1.0          0.0    1.0   \n",
      "3                         1.0           1.0           1.0          0.0    1.0   \n",
      "4                         1.0           1.0           1.0          0.0    1.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          1.0           0.0               1.0   \n",
      "1            0.0        0.0          1.0           0.0               1.0   \n",
      "2            1.0        1.0          1.0           0.0               1.0   \n",
      "3            1.0        1.0          1.0           0.0               1.0   \n",
      "4            1.0        1.0          1.0           0.0               1.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            1.0       1.0              0.0         0.0   \n",
      "1            1.0       1.0              0.0         0.0   \n",
      "2            1.0       1.0              0.0         0.0   \n",
      "3            1.0       1.0              0.0         0.0   \n",
      "4            0.0       1.0              0.0         0.0   \n",
      "\n",
      "                                            captions  \n",
      "0  no evidence of consolidation to suggest pneumo...  \n",
      "1  no evidence of consolidation to suggest pneumo...  \n",
      "2  there are moderate bilateral pleural effusions...  \n",
      "3  there are moderate bilateral pleural effusions...  \n",
      "4  moderate to large bilateral pleural effusions ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test: 19.770 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.47it/s]\n",
      "100%|██████████| 305/305 [00:50<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         0.0           0.0           0.0          0.0    0.0   \n",
      "1                         0.0           0.0           0.0          0.0    0.0   \n",
      "2                         1.0           1.0           1.0          1.0    1.0   \n",
      "3                         0.0           0.0           0.0          0.0    0.0   \n",
      "4                         0.0           0.0           0.0          0.0    0.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          0.0           0.0               0.0   \n",
      "1            0.0        0.0          0.0           0.0               0.0   \n",
      "2            1.0        1.0          1.0           0.0               1.0   \n",
      "3            0.0        1.0          0.0           0.0               0.0   \n",
      "4            0.0        0.0          0.0           0.0               0.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            0.0       0.0              0.0         1.0   \n",
      "1            0.0       1.0              0.0         1.0   \n",
      "2            0.0       0.0              1.0         0.0   \n",
      "3            0.0       1.0              0.0         1.0   \n",
      "4            0.0       0.0              0.0         1.0   \n",
      "\n",
      "                                            captions  \n",
      "0  the heart size is normal . the hilar and media...  \n",
      "1  streaky bibasilar opacities are most atelectas...  \n",
      "2  slight interval improvement in the interstitia...  \n",
      "3  the lung volumes are normal . better seen on t...  \n",
      "4  there is no change . normal lung volumes . nor...  \n",
      "   Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  Edema  \\\n",
      "0                         0.0           0.0           0.0          0.0    0.0   \n",
      "1                         0.0           0.0           0.0          0.0    0.0   \n",
      "2                         0.0           0.0           0.0          0.0    0.0   \n",
      "3                         0.0           0.0           0.0          0.0    0.0   \n",
      "4                         0.0           0.0           0.0          0.0    0.0   \n",
      "\n",
      "   Consolidation  Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  \\\n",
      "0            0.0        0.0          0.0           0.0               0.0   \n",
      "1            0.0        0.0          0.0           0.0               0.0   \n",
      "2            0.0        0.0          0.0           0.0               0.0   \n",
      "3            0.0        0.0          0.0           0.0               0.0   \n",
      "4            0.0        0.0          0.0           0.0               0.0   \n",
      "\n",
      "   Pleural Other  Fracture  Support Devices  No Finding  \\\n",
      "0            0.0       0.0              0.0         0.0   \n",
      "1            0.0       0.0              0.0         0.0   \n",
      "2            0.0       0.0              0.0         1.0   \n",
      "3            0.0       0.0              0.0         1.0   \n",
      "4            0.0       0.0              0.0         1.0   \n",
      "\n",
      "                                            captions  \n",
      "0  lateral view somewhat limited due to overlying...  \n",
      "1  lateral view somewhat limited due to overlying...  \n",
      "2  frontal and lateral radiographs of the chest a...  \n",
      "3  frontal and lateral radiographs of the chest a...  \n",
      "4  frontal and lateral radiographs of the chest a...  \n"
     ]
    }
   ],
   "source": [
    "e2e_chexpertify(OneNearestNeighbor, 0, 128, load=True)\n",
    "e2e_chexpertify(OneNearestNeighbor, 0, 256, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_df, val_true_df, test_pred_df, test_true_df = load_chexpertify_results(OneNearestNeighborCoarse2Fine, 0, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rightsided terminates in the low svc without e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the cardiac silhouette is mildly enlarged but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>compared with prior there has been no signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>blunting at the left costophrenic may represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>there is no change . and rightsided chest tube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>multiple bilateral focal concerning for pneumo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>there is a persistent lower lingular opacifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the patient is status post right upper lobe re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>frontal and lateral views of the chest were ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the mild pulmonary edema on the last radiograp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  Lung Lesion  \\\n",
       "0                            0.0           0.0           0.0          0.0   \n",
       "1                            1.0           1.0           1.0          0.0   \n",
       "2                            1.0           1.0           1.0          0.0   \n",
       "3                            1.0           1.0           1.0          0.0   \n",
       "4                            1.0           1.0           1.0          1.0   \n",
       "...                          ...           ...           ...          ...   \n",
       "2080                         0.0           0.0           0.0          0.0   \n",
       "2081                         0.0           0.0           1.0          1.0   \n",
       "2082                         0.0           0.0           1.0          1.0   \n",
       "2083                         0.0           0.0           0.0          0.0   \n",
       "2084                         1.0           0.0           1.0          0.0   \n",
       "\n",
       "      Edema  Consolidation  Pneumonia  Atelectasis  Pneumothorax  \\\n",
       "0       0.0            0.0        0.0          0.0           0.0   \n",
       "1       0.0            1.0        1.0          1.0           0.0   \n",
       "2       1.0            1.0        1.0          0.0           0.0   \n",
       "3       0.0            0.0        0.0          1.0           0.0   \n",
       "4       1.0            1.0        1.0          0.0           0.0   \n",
       "...     ...            ...        ...          ...           ...   \n",
       "2080    0.0            0.0        1.0          0.0           0.0   \n",
       "2081    0.0            1.0        1.0          1.0           0.0   \n",
       "2082    0.0            1.0        1.0          1.0           1.0   \n",
       "2083    0.0            0.0        0.0          0.0           0.0   \n",
       "2084    0.0            1.0        1.0          1.0           0.0   \n",
       "\n",
       "      Pleural Effusion  Pleural Other  Fracture  Support Devices  No Finding  \\\n",
       "0                  0.0            0.0       0.0              1.0         0.0   \n",
       "1                  1.0            1.0       1.0              0.0         0.0   \n",
       "2                  1.0            1.0       1.0              0.0         0.0   \n",
       "3                  1.0            0.0       1.0              0.0         0.0   \n",
       "4                  0.0            0.0       0.0              1.0         0.0   \n",
       "...                ...            ...       ...              ...         ...   \n",
       "2080               0.0            1.0       1.0              0.0         1.0   \n",
       "2081               0.0            1.0       1.0              0.0         0.0   \n",
       "2082               1.0            1.0       1.0              0.0         0.0   \n",
       "2083               0.0            1.0       1.0              0.0         1.0   \n",
       "2084               1.0            1.0       1.0              0.0         0.0   \n",
       "\n",
       "                                               captions  \n",
       "0     rightsided terminates in the low svc without e...  \n",
       "1     the cardiac silhouette is mildly enlarged but ...  \n",
       "2     compared with prior there has been no signific...  \n",
       "3     blunting at the left costophrenic may represen...  \n",
       "4     there is no change . and rightsided chest tube...  \n",
       "...                                                 ...  \n",
       "2080  multiple bilateral focal concerning for pneumo...  \n",
       "2081  there is a persistent lower lingular opacifica...  \n",
       "2082  the patient is status post right upper lobe re...  \n",
       "2083  frontal and lateral views of the chest were ob...  \n",
       "2084  the mild pulmonary edema on the last radiograp...  \n",
       "\n",
       "[2085 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test from remote run on server\n",
    "\n",
    "Requirements: `predicted_val_captions.npy` and `predicted_test_captions.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:52<00:00,  3.29it/s]\n",
      "100%|██████████| 174/174 [01:05<00:00,  2.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <td>0.729483</td>\n",
       "      <td>0.658436</td>\n",
       "      <td>0.692141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <td>0.657244</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>0.634812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Opacity</th>\n",
       "      <td>0.744152</td>\n",
       "      <td>0.712885</td>\n",
       "      <td>0.728183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Lesion</th>\n",
       "      <td>0.266355</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.306452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edema</th>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.550562</td>\n",
       "      <td>0.575793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>0.620123</td>\n",
       "      <td>0.683258</td>\n",
       "      <td>0.650161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>0.438119</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.474531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>0.624299</td>\n",
       "      <td>0.660079</td>\n",
       "      <td>0.641691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.158940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <td>0.640426</td>\n",
       "      <td>0.650108</td>\n",
       "      <td>0.645230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Other</th>\n",
       "      <td>0.422819</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fracture</th>\n",
       "      <td>0.555738</td>\n",
       "      <td>0.559406</td>\n",
       "      <td>0.557566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Devices</th>\n",
       "      <td>0.632135</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.659316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.836036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro</th>\n",
       "      <td>0.564656</td>\n",
       "      <td>0.582714</td>\n",
       "      <td>0.572204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro</th>\n",
       "      <td>0.634371</td>\n",
       "      <td>0.649143</td>\n",
       "      <td>0.641672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                       Recall  Precision        F1\n",
       "Enlarged Cardiomediastinum  0.729483   0.658436  0.692141\n",
       "Cardiomegaly                0.657244   0.613861  0.634812\n",
       "Lung Opacity                0.744152   0.712885  0.728183\n",
       "Lung Lesion                 0.266355   0.360759  0.306452\n",
       "Edema                       0.603448   0.550562  0.575793\n",
       "Consolidation               0.620123   0.683258  0.650161\n",
       "Pneumonia                   0.438119   0.517544  0.474531\n",
       "Atelectasis                 0.624299   0.660079  0.641691\n",
       "Pneumothorax                0.148148   0.171429  0.158940\n",
       "Pleural Effusion            0.640426   0.650108  0.645230\n",
       "Pleural Other               0.422819   0.480916  0.450000\n",
       "Fracture                    0.555738   0.559406  0.557566\n",
       "Support Devices             0.632135   0.688940  0.659316\n",
       "No Finding                  0.822695   0.849817  0.836036\n",
       "Macro                       0.564656   0.582714  0.572204\n",
       "Micro                       0.634371   0.649143  0.641672"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_captions = np.load('remote_server/predicted_val_captions.npy')\n",
    "until = pred_captions.shape[0]\n",
    "true_captions = np.load('mimic_cxr/raw_embeddings/val/captions_0.npy')[:until]\n",
    "evaluate_clinical(true_captions, pred_captions, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:15<00:00,  2.72it/s]\n",
      "100%|██████████| 42/42 [00:09<00:00,  4.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <td>0.746429</td>\n",
       "      <td>0.741135</td>\n",
       "      <td>0.743772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <td>0.691304</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.685345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Opacity</th>\n",
       "      <td>0.784722</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.758389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Lesion</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.259067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edema</th>\n",
       "      <td>0.565934</td>\n",
       "      <td>0.559783</td>\n",
       "      <td>0.562842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>0.515306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.469841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>0.610465</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.541237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <td>0.527607</td>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.529231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Other</th>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.614754</td>\n",
       "      <td>0.531915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fracture</th>\n",
       "      <td>0.598086</td>\n",
       "      <td>0.606796</td>\n",
       "      <td>0.602410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Devices</th>\n",
       "      <td>0.597087</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>0.672131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>0.581197</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.541833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro</th>\n",
       "      <td>0.524098</td>\n",
       "      <td>0.559891</td>\n",
       "      <td>0.536325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro</th>\n",
       "      <td>0.581012</td>\n",
       "      <td>0.615033</td>\n",
       "      <td>0.597539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                       Recall  Precision        F1\n",
       "Enlarged Cardiomediastinum  0.746429   0.741135  0.743772\n",
       "Cardiomegaly                0.691304   0.679487  0.685345\n",
       "Lung Opacity                0.784722   0.733766  0.758389\n",
       "Lung Lesion                 0.200000   0.367647  0.259067\n",
       "Edema                       0.565934   0.559783  0.562842\n",
       "Consolidation               0.480952   0.554945  0.515306\n",
       "Pneumonia                   0.413408   0.544118  0.469841\n",
       "Atelectasis                 0.610465   0.486111  0.541237\n",
       "Pneumothorax                0.071429   0.142857  0.095238\n",
       "Pleural Effusion            0.527607   0.530864  0.529231\n",
       "Pleural Other               0.468750   0.614754  0.531915\n",
       "Fracture                    0.598086   0.606796  0.602410\n",
       "Support Devices             0.597087   0.768750  0.672131\n",
       "No Finding                  0.581197   0.507463  0.541833\n",
       "Macro                       0.524098   0.559891  0.536325\n",
       "Micro                       0.581012   0.615033  0.597539"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_captions = np.load('remote_server/predicted_test_captions.npy')\n",
    "until = pred_captions.shape[0]\n",
    "true_captions = np.load('mimic_cxr/raw_embeddings/test/captions_0.npy')[:until]\n",
    "evaluate_clinical(true_captions, pred_captions, batch_size=12)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5b2b1eb0b4180dccfbf3e31c6f887b3086317eead5a7f5406a6958b92b74dfc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('research_rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
